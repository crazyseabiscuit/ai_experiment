{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8b5249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheny\\AppData\\Local\\Temp\\ipykernel_16748\\4062287425.py:29: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m我需要使用搜索工具来查找当前北京的温度，然后使用计算器工具来计算这个温度的1/4。  \n",
      "Action: Search  \n",
      "Action Input: 当前北京的温度是多少  \n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m{'type': 'weather_result', 'temperature': '89', 'unit': 'Fahrenheit', 'precipitation': '0%', 'humidity': '40%', 'wind': '5 mph', 'location': '中国北京市', 'date': 'Thursday 9:00 AM', 'weather': 'Sunny'}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m当前北京的温度是89华氏度。接下来，我将计算这个温度的1/4。  \n",
      "Action: Calculator  \n",
      "Action Input: 89 / 4  \n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 22.25\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m当前北京的温度是89华氏度，这个温度的1/4是22.25华氏度。\n",
      "\n",
      "Final Answer: 当前北京的温度是89华氏度，这个温度的1/4是22.25华氏度。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': '当前北京的温度是多少华氏度？这个温度的1/4是多少', 'output': '当前北京的温度是89华氏度，这个温度的1/4是22.25华氏度。'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain_community.llms import Tongyi  # 导入通义千问Tongyi模型\n",
    "from langchain.agents import AgentType\n",
    "import dashscope\n",
    "# 你需要在环境变量中添加 OPENAI_API_KEY 和 SERPAPI_API_KEY\n",
    "#os.environ[\"OPENAI_API_KEY\"] = '*******'\n",
    "#os.environ[\"SERPAPI_API_KEY\"] = '*******'\n",
    " \n",
    "# 从环境变量获取 dashscope 的 API Key\n",
    "api_key = os.environ.get('DASHSCOPE_API_KEY')\n",
    "dashscope.api_key = api_key\n",
    " \n",
    "# 加载 Tongyi 模型\n",
    "llm = Tongyi(model_name=\"qwen-turbo\", dashscope_api_key=api_key)  # 使用通义千问qwen-turbo模型\n",
    " \n",
    "# 加载 serpapi, llm-math工具, 因为llm-math要使用LLM，所以后面需要指定LLM\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    " \n",
    "\"\"\"\n",
    "agent：代理类型  \n",
    "    zero-shot-react-description: 根据工具的描述和请求内容的来决定使用哪个工具（最常用）\n",
    "    react-docstore: 使用 ReAct 框架和 docstore 交互, 使用Search 和Lookup 工具, 前者用来搜, 后者寻找term, 举例: Wipipedia 工具\n",
    "    self-ask-with-search 此代理只使用一个工具: Intermediate Answer, 它会为问题寻找事实答案(指的非 gpt 生成的答案, 而是在网络中,文本中已存在的), 如 Google search API 工具\n",
    "    conversational-react-description: 为会话设置而设计的代理, 它的prompt会被设计的具有会话性, 且还是会使用 ReAct 框架来决定使用来个工具, 并且将过往的会话交互存入内存\n",
    "\"\"\"\n",
    "# 工具加载后需要初始化，verbose=True 代表打印执行详情\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    " \n",
    "result = agent.invoke({\"input\": \"当前北京的温度是多少华氏度？这个温度的1/4是多少\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8890f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package langchain_community.llms in langchain_community:\n",
      "\n",
      "NAME\n",
      "    langchain_community.llms\n",
      "\n",
      "DESCRIPTION\n",
      "    **LLM** classes provide\n",
      "    access to the large language model (**LLM**) APIs and services.\n",
      "    \n",
      "    **Class hierarchy:**\n",
      "    \n",
      "    .. code-block::\n",
      "    \n",
      "        BaseLanguageModel --> BaseLLM --> LLM --> <name>  # Examples: AI21, HuggingFaceHub, OpenAI\n",
      "    \n",
      "    **Main helpers:**\n",
      "    \n",
      "    .. code-block::\n",
      "    \n",
      "        LLMResult, PromptValue,\n",
      "        CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun,\n",
      "        CallbackManager, AsyncCallbackManager,\n",
      "        AIMessage, BaseMessage\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    ai21\n",
      "    aleph_alpha\n",
      "    amazon_api_gateway\n",
      "    anthropic\n",
      "    anyscale\n",
      "    aphrodite\n",
      "    arcee\n",
      "    aviary\n",
      "    azureml_endpoint\n",
      "    baichuan\n",
      "    baidu_qianfan_endpoint\n",
      "    bananadev\n",
      "    baseten\n",
      "    beam\n",
      "    bedrock\n",
      "    bigdl_llm\n",
      "    bittensor\n",
      "    cerebriumai\n",
      "    chatglm\n",
      "    chatglm3\n",
      "    clarifai\n",
      "    cloudflare_workersai\n",
      "    cohere\n",
      "    ctransformers\n",
      "    ctranslate2\n",
      "    databricks\n",
      "    deepinfra\n",
      "    deepsparse\n",
      "    edenai\n",
      "    exllamav2\n",
      "    fake\n",
      "    fireworks\n",
      "    forefrontai\n",
      "    friendli\n",
      "    gigachat\n",
      "    google_palm\n",
      "    gooseai\n",
      "    gpt4all\n",
      "    gradient_ai\n",
      "    huggingface_endpoint\n",
      "    huggingface_hub\n",
      "    huggingface_pipeline\n",
      "    huggingface_text_gen_inference\n",
      "    human\n",
      "    ipex_llm\n",
      "    javelin_ai_gateway\n",
      "    koboldai\n",
      "    konko\n",
      "    layerup_security\n",
      "    llamacpp\n",
      "    llamafile\n",
      "    loading\n",
      "    manifest\n",
      "    minimax\n",
      "    mlflow\n",
      "    mlflow_ai_gateway\n",
      "    mlx_pipeline\n",
      "    modal\n",
      "    moonshot\n",
      "    mosaicml\n",
      "    nlpcloud\n",
      "    oci_data_science_model_deployment_endpoint\n",
      "    oci_generative_ai\n",
      "    octoai_endpoint\n",
      "    ollama\n",
      "    opaqueprompts\n",
      "    openai\n",
      "    openllm\n",
      "    openlm\n",
      "    outlines\n",
      "    pai_eas_endpoint\n",
      "    petals\n",
      "    pipelineai\n",
      "    predibase\n",
      "    predictionguard\n",
      "    promptlayer_openai\n",
      "    replicate\n",
      "    rwkv\n",
      "    sagemaker_endpoint\n",
      "    sambanova\n",
      "    self_hosted\n",
      "    self_hosted_hugging_face\n",
      "    solar\n",
      "    sparkllm\n",
      "    stochasticai\n",
      "    symblai_nebula\n",
      "    textgen\n",
      "    titan_takeoff\n",
      "    together\n",
      "    tongyi\n",
      "    utils\n",
      "    vertexai\n",
      "    vllm\n",
      "    volcengine_maas\n",
      "    watsonxllm\n",
      "    weight_only_quantization\n",
      "    writer\n",
      "    xinference\n",
      "    yandex\n",
      "    yi\n",
      "    you\n",
      "    yuan2\n",
      "\n",
      "FUNCTIONS\n",
      "    __getattr__(name: str) -> Any\n",
      "\n",
      "DATA\n",
      "    __all__ = ['AI21', 'AlephAlpha', 'AmazonAPIGateway', 'Anthropic', 'Any...\n",
      "\n",
      "FILE\n",
      "    d:\\appdata\\local\\programs\\python\\python311\\lib\\site-packages\\langchain_community\\llms\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#help(load_tools)\n",
    "import langchain_community\n",
    "help(langchain_community.llms)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
